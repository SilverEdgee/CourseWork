# Распознавание жестов рук с использованием MediaPipe

Проект для распознавания жестов рук и управления компьютером с помощью библиотеки MediaPipe и машинного обучения.

![Демонстрация работы](https://user-images.githubusercontent.com/37477845/102222442-c452cd00-3f26-11eb-93ec-c387c98231be.gif)

## Возможности

- Распознавание 9 основных жестов рук (открытая ладонь, кулак, указательный палец и т.д.)
- Распознавание динамических жестов (движений пальцев)
- Привязка действий к распознанным жестам (запуск программ, нажатие клавиш, управление системой)
- Удобный графический интерфейс на PyQt5
- Возможность настройки под себя

## Требования

- Python 3.7 или выше
- MediaPipe 0.8.1 или выше
- OpenCV 3.4.2 или выше
- PyQt5 5.15.0 или выше
- TensorFlow 2.3.0 или выше

## Установка

```bash
# Клонирование репозитория
git clone https://github.com/SilverEdgee/CourseWork.git
cd CourseWork

# Установка зависимостей
pip install -r requirements.txt
```

## Использование

### Запуск графического интерфейса

```bash
python qt_app.py
```

### Запуск консольной версии

```bash
python app.py
```

Возможные аргументы командной строки:
- `--camera` - номер камеры (по умолчанию: 0)
- `--width` - ширина изображения (по умолчанию: 640)
- `--height` - высота изображения (по умолчанию: 480)
- `--disable-pyautogui` - отключение функциональности pyautogui (для работы без X-сервера)

## Структура проекта

```
├── app.py                      # Консольная версия приложения
├── qt_app.py                   # Запуск GUI приложения
├── qt_gui.py                   # Главный файл GUI интерфейса
├── gesture_processor.py        # Обработка и распознавание жестов
├── gesture_actions.py          # Выполнение действий по жестам
├── gesture_actions_config.json # Конфигурация привязок жестов
├── project_structure.txt       # Подробное описание структуры проекта
├── usage_guide.txt             # Руководство пользователя
├── model/                      # Модели машинного обучения
│   ├── keypoint_classifier/    # Классификатор ключевых точек
│   │   ├── keypoint.csv        # Данные для обучения
│   │   ├── keypoint_classifier.tflite # Обученная модель
│   │   ├── keypoint_classifier_label.csv # Метки классов
│   │   └── keypoint_classifier.py   # Код классификатора
│   └── point_history_classifier/    # Классификатор движений
│       ├── point_history.csv  # Данные для обучения
│       ├── point_history_classifier.tflite # Обученная модель
│       ├── point_history_classifier_label.csv # Метки классов
│       └── point_history_classifier.py # Код классификатора
├── utils/                     # Вспомогательные утилиты
│   └── cvfpscalc.py          # Расчет FPS
├── keypoint_classification_EN.ipynb # Англзычная версия ноутбука
└── point_history_classification.ipynb # Ноутбук для обучения модели движений
```

## Обучение моделей

### Обучение модели распознавания жестов

1. Запустите `app.py` и нажмите 'k' для переключения в режим сбора данных о жестах
2. Нажимайте клавиши от 0 до 9 для записи соответствующих классов жестов
3. Откройте и выполните ноутбук `keypoint_classification.ipynb`

### Обучение модели распознавания движений

1. Запустите `app.py` и нажмите 'h' для переключения в режим сбора данных о движениях
2. Нажимайте клавиши от 0 до 9 для записи соответствующих классов движений
3. Откройте и выполните ноутбук `point_history_classification.ipynb`

## Доступные жесты и их действия

- Open (Открытая ладонь) - Tab - Подтверждение автодополнения кода
- Close (Закрытая ладонь) - Ctrl+X - Вырезать выделенный текст
- Pointer (Указывающий палец) - Home - Перемещение в начало строки
- OK (Жест "ОК") - Shift+End - Выделение от курсора до конца строки
- Thumb Up (Большой палец вверх) - Ctrl+S - Сохранить файл
- Peace Sign (Знак мира/победы) - Ctrl+C - Копирование выделенного текста
- Thumb Down (Большой палец вниз) - Ctrl+Z - Отменить последнее действие
- Rock - F5 - Запуск отладки
- Pistol - Ctrl+/ - Закомментировать/раскомментировать строку

## Настройка действий

Вы можете настроить действия, которые будут выполняться при распознавании определенных жестов, редактируя файл `gesture_actions_config.json` или используя графический интерфейс.